|Paper Name                                                                                                                                                        |Year|Authors                                                                                                                                                                                     |Journal/Conference                                                              |Citations|DOI                                                                                                                               |Code Availability|Code                                                                                                                          |CL Abuse Type                                               |Code mixed? (paper)|Model Type                                                                  |Transfer Level                            |CLTL Approach                                                                                                                                                                                                                                                                                                                           |Language Pairs                                                                                                                                                                                                                                                                                                                                                                                 |Metric                                                           |
|------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------|---------|----------------------------------------------------------------------------------------------------------------------------------|-----------------|------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------|-------------------|----------------------------------------------------------------------------|------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------|
|COVID-19 and cyberbullying: deep ensemble model to identify cyberbullying from code-switched languages during the pandemic                                        |2023|Jyoti Prakash Singh, Sayanta Paul, Sriparna Saha                                                                                                                                            |Multimedia Tools and Applications                                               |10       |https://link.springer.com/article/10.1007/s11042-021-11601-9                                                                      |No               |                                                                                                                              |cyberbullying                                               |yes                |BERT, CNN, LR, LSTM, MLP, SVM                                               |feature-level                             |propose an ensemble model of deep neural networks (MLP, CNN, BiLSTM and BERT) with random and Xavier initialization of weights based on aligned word embeddings in source and target languages                                                                                                                                          |EN-HI                                                                                                                                                                                                                                                                                                                                                                                          |macro F1                                                         |
|Model-Agnostic Meta-Learning for Multilingual Hate Speech Detection                                                                                               |2023|Eshaan Tanwar, Md Rabiul Awal, Roy Ka-Wei Lee, Tanmay Garg, Tanmoy Chakraborty                                                                                                              |IEEE Computational Social Systems                                               |2        |https://ieeexplore.ieee.org/document/10100717                                                                                     |No               |                                                                                                                              |hate speech                                                 |no                 |XLM-R, mBERT                                                                |parameter-level                           |propose HateMAML, a model-agnostic meta-learning-based framework that use a semi-supervised self-refinement strategy to fine-tune a better pre-trained model for unseen data in target language                                                                                                                                         |EN-AR, EN-DA, EN-DE, EN-EL, EN-ES, EN-HI, EN-IT, EN-TR                                                                                                                                                                                                                                                                                                                                         |F1                                                               |
|Label modification and bootstrapping for zero-shot cross-lingual hate speech detection                                                                            |2023|Alexander Fraser, Irina Bigoulaeva, Iryna Gurevych, Viktor Hangya                                                                                                                           |LREC                                                                            |2        |https://link.springer.com/content/pdf/10.1007/s10579-023-09637-4.pdf                                                              |No               |                                                                                                                              |hate speech                                                 |no                 |CNN, LSTM, mBERT                                                            |feature-level, parameter-level            |propose a ensemble-based cross-lingual approach by leveraging cross-lingual word embeddings to train CNN/BiLSTM classifiers and directly train mBERT on English dataset, generating pseudo labels for two unlabelled German datasets by an ensemble of three trained models, and fine-tuning them on bootstrapping German datasets      |EN-DE                                                                                                                                                                                                                                                                                                                                                                                          |macro F1                                                         |
|SexWEs: Domain-Aware Word Embeddings via Cross-lingual Semantic Specialisation for Chinese Sexism Detection in Social Media                                       |2023|Aiqi Jiang, Arkaitz Zubiaga                                                                                                                                                                 |ICWSM                                                                           |1        |https://ojs.aaai.org/index.php/ICWSM/article/view/22159                                                                           |Yes              |https://github.com/aggiejiang/SexWEs                                                                                          |sexism                                                      |no                 |BERT, CNN, MacBERT                                                          |feature-level                             |propose a domain-aware cross-lingual semantic specialisation framework between source and target languages to construct sexism-specific word embeddings (SexWEs)                                                                                                                                                                        |EN-ZH                                                                                                                                                                                                                                                                                                                                                                                          |macro F1, correlation score                                      |
|Respectful or Toxic? Using Zero-Shot Learning with Language Models to Detect Hate Speech                                                                          |2023|Debora Nozza, Dirk Hovy, Flor Miriam Plaza-del-Arco                                                                                                                                         |WOAH Workshop                                                                   |1        |https://aclanthology.org/2023.woah-1.6/                                                                                           |Yes              |https://github.com/MilaNLProc/prompting_hate_speech                                                                           |toxicity                                                    |no                 |BERT, DeBERTa, FLAN-T5, RoBERTa, XLM-R, mT0                                 |parameter-level                           |explore different prompting formats on multiple hate speech datasets, andcompare the zero-shot learning performance of encoder models with the recent LLMs based on instruction fine-tuning                                                                                                                                             |EN-ES, EN-IT                                                                                                                                                                                                                                                                                                                                                                                   |macro F1                                                         |
|Cross-Cultural Transfer Learning for Chinese Offensive Language Detection                                                                                         |2023|Daniel Hershcovich, Laura Cabello, Li Zhou, Yong Cao                                                                                                                                        |C3NLP Workshop                                                                  |0        |https://aclanthology.org/2023.c3nlp-1.2/                                                                                          |No               |                                                                                                                              |offensive                                                   |no                 |BERT, RoBERTa, XLM-R, mBERT                                                 |data-level, parameter-level               |investigate the impact of cultural background differences based on Korean / English and Chinese languages in zero- / few-shot  and translation settings                                                                                                                                                                                 |EN-ZH, KO-ZH                                                                                                                                                                                                                                                                                                                                                                                   |F1, accuracy                                                     |
|Multi-input integrative learning using deep neural networks and transfer learning for cyberbullying detection in real-time code-mix data                          |2022|Akshi Kumar, Nitin Sachdeva                                                                                                                                                                 |Multimedia Systems                                                              |35       |https://link.springer.com/article/10.1007/s00530-020-00672-7                                                                      |No               |                                                                                                                              |cyberbullying                                               |yes                |Capsule Network, LSTM, MLP                                                  |data-level, parameter-level               |propose MIIL-DNN, a multi-input integrative learning framework based on deep neural networks, combining information from three paralleled sub-networks by using model-level multi-lingual fusion strategy to detect English-Hindi code-mixed bully content                                                                              |EN-HI                                                                                                                                                                                                                                                                                                                                                                                          |AUC curve                                                        |
|A multilingual offensive language detection method based on transfer learning from transformer fine-tuning model                                                  |2022|Fatima-zahra El-Alami, Noureddine En Nahnahi, Said Ouatik El Alaoui                                                                                                                         |King Saud University - Computer and Information Sciences                        |17       |https://doi.org/10.1016/j.jksuci.2021.07.013                                                                                      |No               |                                                                                                                              |offensive                                                   |no                 |AraBERT, BERT, mBERT                                                        |data-level, parameter-level               |propose a joint learning framework by fine-tuning mBERT on mixed datasets, and translation-based methods using BERT and AraBERT for English and Arabic languages                                                                                                                                                                        |EN-AR                                                                                                                                                                                                                                                                                                                                                                                          |accuracy, F1                                                     |
|Data Bootstrapping Approaches to Improve Low Resource Abusive Language Detection for Indic Languages                                                              |2022|Animesh Mukherjee, Mithun Das, Somnath Banerjee                                                                                                                                             |ACM HT                                                                          |13       |https://dl.acm.org/doi/pdf/10.1145/3511095.3531277                                                                                |No               |                                                                                                                              |abusive                                                     |no                 |MuRIL(MU) - multilingual Indic, mBERT                                       |data-level, parameter-level               |perform a large-scale analysis of cross-lingual hate speech by investigating the performance of multilingual models (mBERT and MuRIL) on four different transfer strategies across eight different Indic languages                                                                                                                      |EN-BN, EN-HI, EN-KA, EN-MA, EN-MR, EN-TA, EN-UR                                                                                                                                                                                                                                                                                                                                                |accuracy, macro F1                                               |
|Multilingual HateCheck: Functional Tests for Multilingual Hate Speech Detection Models                                                                            |2022|Bertie Vidgen, Debora Nozza, Haitham Seelawi, Paul Röttger, Zeerak Talat                                                                                                                    |WOAH Workshop                                                                   |11       |https://aclanthology.org/2022.woah-1.15/                                                                                          |Yes              |https://github.com/rewire-online/multilingual-hatecheck                                                                       |hate speech                                                 |no                 |XLM-T                                                                       |parameter-level                           |propose Multilingual HateCheck (MHC), a suite of functional tests for multilingual hate speech detection models, and fine-tune multilingual XLM-T on individual datasets and combined datasets in Spanish, Italian and Portuguese.                                                                                                      |EN-ES, EN-IT, EN-PT                                                                                                                                                                                                                                                                                                                                                                            |macro F1                                                         |
|Improving Zero-Shot Cross-Lingual Hate Speech Detection with Pseudo-Label Fine-Tuning of Transformer Language Models                                              |2022|Arkaitz Zubiaga, Gareth Tyson, Haris Bin Zia, Ignacio Castro                                                                                                                                |ICWSM                                                                           |10       |https://ojs.aaai.org/index.php/ICWSM/article/view/19402                                                                           |Yes              |https://github.com/harisbinzia/ZeroshotCrosslingualHateSpeech                                                                 |hate speech                                                 |no                 |BERT, RoBERTa, XLM-R                                                        |data-level, parameter-level               |propose a cross-lingual transfer approach by training XLM-R in a zero-shot setting to generate pseudo-labels for target data, and then using it to fine-tune monolingual pre-trained models                                                                                                                                             |EN-AR, EN-DE, EN-EL, EN-ES, EN-IT, EN-TR                                                                                                                                                                                                                                                                                                                                                       |macro F1                                                         |
|Cross-Lingual Few-Shot Hate Speech and Offensive Language Detection Using Meta Learning                                                                           |2022|Marzieh Mozafari, Noel Crespi, Reza Farahbakhsh                                                                                                                                             |IEEE Access                                                                     |9        |https://ieeexplore.ieee.org/abstract/document/9696324                                                                             |No               |                                                                                                                              |hate speech, offensive                                      |no                 |XLM-R                                                                       |parameter-level                           |propose a cross-lingual meta learning-based approach to fine-tuning the base learner XLM-R with parallel few-shot datasets in different target languages by using optimisation-based Model-Agnostic Meta-Learning (MAML) and  Proto-MAML models                                                                                         |EN-AR, EN-DA, EN-DE, EN-ES, EN-FA, EN-FR, EN-GR, EN-ID, EN-IT, EN-PT, EN-TR                                                                                                                                                                                                                                                                                                                    |average F1                                                       |
|Transfer language selection for zero-shot cross-lingual abusive language detection                                                                                |2022|Fumito Masui, Gniewosz Leliwa, Juuso Eronen, Masaki Arata, Michal Ptaszynski, Michal Wroczynski                                                                                             |Information Processing and Management                                           |8        |https://doi.org/10.1016/j.ipm.2022.102981                                                                                         |No               |                                                                                                                              |abusive                                                     |no                 |XLM-R, mBERT                                                                |parameter-level                           |select optimal transfer languages based on correlation between linguistic similarity and zero-shot cross-lingual performance of mBERT/XLM-R on 7 different languages, and propose a new linguistic similarity metric based on WALS                                                                                                      |EN-DA, EN-DE, EN-JA, EN-KO, EN-PL, EN-RU                                                                                                                                                                                                                                                                                                                                                       |macro F1, pearson/spearman correlation(similarity)               |
|Highly Generalizable Models for Multilingual Hate Speech Detection                                                                                                |2022|Neha Deshpande, Nicholas Farris, Vidhur Kumar                                                                                                                                               |Arxiv                                                                           |4        |https://arxiv.org/abs/2201.11294                                                                                                  |Yes              |https://github.com/vidhur2k/Multilngual-Hate-Speech                                                                           |hate speech                                                 |no                 |CNN-GRU, LR, mBERT                                                          |data-level, parameter-level               |propose multilingual experiments for a compiled dataset of 11 languages by training the model on all available languages or a particular language family, and testing it on each language (in the family)                                                                                                                               |AR-HI, AR-ID, AR-PT, AR-TR, DA-AR, DA-HI, DA-ID, DA-PT, DA-TR, DE-AR, DE-DA, DE-ES, DE-FR, DE-HI, DE-ID, DE-IT, DE-PT, DE-TR, EN-AR, EN-DA, EN-DE, EN-ES, EN-FR, EN-HI, EN-ID, EN-IT, EN-PT, EN-TR, ES-AR, ES-DA, ES-HI, ES-ID, ES-IT, ES-PT, ES-TR, FR-AR, FR-DA, FR-ES, FR-HI, FR-ID, FR-IT, FR-PT, FR-TR, HI-ID, IT-AR, IT-DA, IT-HI, IT-ID, IT-PT, IT-TR, PR-HI, PR-ID, TR-HI, TR-ID, TR-PT|weighted F1                                                      |
|Unsupervised Embeddings with Graph Auto-Encoders for Multi-domain and Multilingual Hate Speech Detection                                                          |2022|Gretel Liz De la Peña Sarracén, Paolo Rosso                                                                                                                                                 |LREC                                                                            |3        |https://aclanthology.org/2022.lrec-1.236/                                                                                         |No               |                                                                                                                              |hate speech                                                 |no                 |GNN, USE, XLM-R, mBERT                                                      |feature-level, parameter-level            |propose a Graph Auto-Encoders (GAE) framework to learn embeddings of a set of texts in an unsupervised way, and add prior language knowledge using Universal Sentences Encoder (USE) in a multilingual setting                                                                                                                          |EN-DE, EN-HR, EN-RU, EN-SQ, EN-TR                                                                                                                                                                                                                                                                                                                                                              |F1                                                               |
|Resources for Multilingual Hate Speech Detection                                                                                                                  |2022|Aymé Arango, Barbara Poblete, Jorge Pérez                                                                                                                                                   |WOAH Workshop                                                                   |3        |https://aclanthology.org/2022.woah-1.12/                                                                                          |Yes              |https://codalab.lisn.upsaclay.fr/competitions/1221?secret_key=c1de3893-de48-4ca1-8071-89e82f189039                            |offensive                                                   |no                 |BERT, CNN-GRU, LR, LSTM, XLM-R                                              |                                          |perform a comparative study of existing cross-lingual architectures on multilingual datasets including self-created Spanish dataset                                                                                                                                                                                                     |EN-ES, ES-EN                                                                                                                                                                                                                                                                                                                                                                                   |F1, ROC                                                          |
|Cross-lingual offensive speech identification with transfer learning for low-resource languages                                                                   |2022|Chun Xu, Fang Chen, Shaolin Zhu, Xiayang Shi, Xinyi Liu, Yuanyuan Huang                                                                                                                     |Computers and Electrical Engineering                                            |3        |https://doi.org/10.1016/j.compeleceng.2022.108005                                                                                 |Yes              |https://github.com/zsl-nlp/DeepOffense-Unsupervised                                                                           |offensive                                                   |no                 |LSTM                                                                        |data-level, feature-level, parameter-level|align BERT embeddings in source and target language into a shared space using adversarial training and Procrustes analysis, and propose an agreement regularised training schema to select source data which is most similar to target one based on shared embeddings to fine-tune the trained LSTM model on new labelled target dataset|EN-AR, EN-DA, EN-EL, EN-TR                                                                                                                                                                                                                                                                                                                                                                     |accuracy, macro F1, macro precision, macro recall                |
|Data-Efficient Strategies for Expanding Hate Speech Detection into Under-Resourced Languages                                                                      |2022|Debora Nozza, Dirk Hovy, Federico Bianchi, Paul Röttger                                                                                                                                     |EMNLP                                                                           |3        |https://aclanthology.org/2022.emnlp-main.383/                                                                                     |Yes              |https://github.com/paul-rottger/efficient-low-resource-hate-detection                                                         |hate speech                                                 |no                 |XLM-T                                                                       |parameter-level                           |fine-tune pre-trained multilingual model XLM-T by using randomly sampled differently-sized datasets in target language                                                                                                                                                                                                                  |EN-AR, EN-ES, EN-HI, EN-IT, EN-PT                                                                                                                                                                                                                                                                                                                                                              |macro F1, R2 for regressions                                     |
|Detecting Hateful and Offensive Speech in Arabic Social Media Using Transfer Learning                                                                             |2022|Karim Gasmi, Mariya Ouaissa, Mariyam Ouaissa, Moez Krichen, Mutiq Almutiq, Zakaria Boulouard                                                                                                |MDPI Applied Science                                                            |2        |https://doi.org/10.3390/app122412823                                                                                              |No               |                                                                                                                              |hate speech                                                 |no                 |AraBERT, BERT, mBERT                                                        |data-level, parameter-level               |propose a selection of BERT-based models in a cross-lingual setting, covering texts written in the standard Arabic, as well as three of most spoken Arabic dialects in the region (Egyptian, Iraqi, and Gulf).                                                                                                                          |EN-AR, between Arabic dialects                                                                                                                                                                                                                                                                                                                                                                 |Accuracy, Precision, Recall, F1-Score, and Confusion Matrix      |
|Addressing the Challenges of Cross-Lingual Hate Speech Detection                                                                                                  |2022|Alexander Fraser, Irina Bigoulaeva, Iryna Gurevych, Viktor Hangya                                                                                                                           |Arxiv                                                                           |2        |https://arxiv.org/abs/2201.05922                                                                                                  |No               |                                                                                                                              |hate speech                                                 |no                 |CNN, LSTM, mBERT                                                            |feature-level, parameter-level            |propose a ensemble-based cross-lingual approach by leveraging cross-lingual word embeddings to train CNN/BiLSTM classifiers and directly train mBERT on English dataset, generating pseudo labels for two unlabelled German datasets by an ensemble of three trained models, and fine-tuning them on bootstrapping German datasets      |EN-DE                                                                                                                                                                                                                                                                                                                                                                                          |macro F1                                                         |
|An exploratory experiment on Hindi, Bengali hate-speech detection and transfer learning using neural networks                                                     |2022|Jan Cloos, Tung Minh Phung                                                                                                                                                                  |Arxiv                                                                           |1        |https://arxiv.org/abs/2201.01997                                                                                                  |No               |                                                                                                                              |hate speech                                                 |no                 |LSTM                                                                        |parameter-level                           |propose a cross-lingual approach between Hindi and Bengali by reusing monolingual Hindi classifier without embedding layers and replacing embedding layer with untrained Bengali embedding layer to build Hindi-Bengali cross-lingual classifier                                                                                        |HI-BN                                                                                                                                                                                                                                                                                                                                                                                          |accuracy, macro F1                                               |
|A Checkpoint on Multilingual Misogyny Identification                                                                                                              |2022|Alberto Barrón-Cedeño, Arianna Muti                                                                                                                                                         |ACL SRW                                                                         |1        |https://aclanthology.org/2022.acl-srw.37/                                                                                         |No               |                                                                                                                              |misogyny                                                    |no                 |BERT, mBERT                                                                 |parameter-level                           |explore the feasibility of detecting misogyny through a transfer learning approach by fine-tuning mBERT in a zero-shot setting or on different  combinations of multiple languages (English, Italian and Spanish)                                                                                                                       |EN-ES, EN-IT, ES-IT                                                                                                                                                                                                                                                                                                                                                                            |F1                                                               |
|Training Multilingual and Adversarial Attack-Robust Models for Hate Detection on Social Media                                                                     |2022|Anastasia Ryzhova, Dmitry Devyatkin, Sergey Volkov, Vladimir Budzko                                                                                                                         |Procedia Computer Science                                                       |1        |https://doi.org/10.1016/j.procs.2022.11.056                                                                                       |No               |                                                                                                                              |hate speech                                                 |no                 |XLM-R                                                                       |data-level, parameter-level               |use multilingual source datasets and an adversarial attacked source dataset to consistently fine-tune an ensemble of XLM-R models and test it on the target dataset in a zero-shot way                                                                                                                                                  |EN-HI, EN-RU                                                                                                                                                                                                                                                                                                                                                                                   |macro F1                                                         |
|Multilingual Auxiliary Tasks Training: Bridging the Gap between Languages for Zero-Shot Transfer of Hate Speech Detection Models                                  |2022|Arij Riabi, Djamé Seddah, Syrielle Montariol                                                                                                                                                |AACL-IJCNLP                                                                     |1        |https://aclanthology.org/2022.findings-aacl.33/                                                                                   |Yes              |https://github.com/ArijRB/Multilingual-Auxiliary-Tasks-Training-Bridging-the-Gap-between-Languages-for-Zero-Shot-Transfer-of-/|hate speech                                                 |no                 |XLM-R, XLM-T, mBERT                                                         |parameter-level                           |propose a zero-shot cross-lingual approach by applying XLM-R to a MACHAMP multi-task architecture to jointly train hate speech detection with auxiliary tasks.                                                                                                                                                                          |EN-ES, EN-IT                                                                                                                                                                                                                                                                                                                                                                                   |macro F1                                                         |
|MACD: Multilingual Abusive Comment Detection at Scale for Indic Languages                                                                                         |2022|Animesh Mukherjee, Binny Mathew, Hastagiri Vanchinathan, Mithun Das, Punyajoy Saha, Somnath Banerjee, Sumegh Roychowdhury, Vikram Gupta                                                     |NeurIPS                                                                         |0        |https://proceedings.neurips.cc//paper_files/paper/2022/hash/a7c4163b33286261b24c72fd3d1707c9-Abstract-Datasets_and_Benchmarks.html|Yes              |https://github.com/ShareChatAI/MACD                                                                                           |abusive                                                     |yes                |AbuseXLMR - multilingual Indic, MuRIL(MU) - multilingual Indic, XLM-R       |parameter-level                           |create multilingual abusive dataset (MACD) and abuse-specific pre-trained model AbuseXLMR, and perform four cross-lingual strategies (zero-shot, few-shot, joint training, and pre-training) across five Indic languages.                                                                                                               |HI-KN, HI-ML, HI-TA, HI-TE                                                                                                                                                                                                                                                                                                                                                                     |accuracy, macro F1                                               |
|Transfer Learning Across Arabic Dialects for Offensive Language Detection                                                                                         |2022|Fatemah Husain, Ozlem Uzuner                                                                                                                                                                |IEEE IALP                                                                       |0        |https://ieeexplore.ieee.org/document/9961263                                                                                      |No               |                                                                                                                              |offensive                                                   |no                 |AraBERT                                                                     |parameter-level                           |explore cross-lingual performance across Arabic dialects (Levantine, Egyptian and Tunisian) by directly fine-tuning AraBERT or keep pre-training it on different combinations of dialects                                                                                                                                               |between Arabic dialects                                                                                                                                                                                                                                                                                                                                                                        |macro-averaged precision, recall, F1, and accuracy               |
|Offensive Content Detection via Synthetic Code-Switched Text                                                                                                      |2022|Cesa Salaam, Danda Rawat, Franck Dernoncourt, Seunghyun Yoon, Trung Bui                                                                                                                     |COLING                                                                          |0        |https://aclanthology.org/2022.coling-1.575/                                                                                       |No               |                                                                                                                              |offensive                                                   |yes                |XLM-R                                                                       |data-level, parameter-level               |release a human-generated dataset for testing for three language combinations en-fr, en-es, and en-de and a synthetic code-switched dataset for multilingual training based on XLM-R model                                                                                                                                              |EN-DE, EN-ES, EN-FR                                                                                                                                                                                                                                                                                                                                                                            |macro F1 score and weighted accuracy                             |
|A Deep Dive into Multilingual Hate Speech Classification                                                                                                          |2021|Animesh Mukherjee, Binny Mathew, Punyajoy Saha, Sai Saketh Aluru                                                                                                                            |ECML-PKDD                                                                       |73       |https://link.springer.com/chapter/10.1007/978-3-030-67670-4_26                                                                    |Yes              |https://github.com/hate-alert/DE-LIMIT                                                                                        |hate speech                                                 |no                 |LR, mBERT                                                                   |data-level, parameter-level               |investigate corss-lingual zero-shot learning by using multi-source languages to train mBERT or LR with LASER embeddings and fine-tuning the trained model in incremental amounts of target data based on 9 languages                                                                                                                    |EN-AR, EN-DE, EN-ES, EN-FR, EN-ID, EN-IT, EN-PL, EN-PT                                                                                                                                                                                                                                                                                                                                         |                                                                 |
|Exploring Stylometric and Emotion-Based Features for Multilingual Cross-Domain Hate Speech Detection                                                              |2021|Darja Fišer, Ilia Markov, Nikola Ljubešić, Walter Daelemans                                                                                                                                 |WASSA Workshop                                                                  |44       |https://aclanthology.org/2021.wassa-1.16/                                                                                         |No               |                                                                                                                              |hate speech                                                 |no                 |BERT, CNN, LSTM                                                             |parameter-level                           |compare the performance of stylometric and emotion-based features with commonly used features and SOTA deep learning models on multilingual hate speech datasets                                                                                                                                                                        |EN-NL, EN-SL                                                                                                                                                                                                                                                                                                                                                                                   |macro F1                                                         |
|A joint learning approach with knowledge injection for zero-shot cross-lingual hate speech detection                                                              |2021|Endang Wahyu Pamungkas, Valerio Basile, Viviana Patti                                                                                                                                       |Information Processing and Management                                           |42       |https://www.sciencedirect.com/science/article/abs/pii/S0306457321000510                                                           |No               |                                                                                                                              |hate speech, offensive                                      |no                 |LSTM, mBERT                                                                 |data-level, parameter-level               |propose a joint-learning cross-lingual approach to detect hate speech， encoding parallel source and target datasets via multilingual representations (MUSE and mBERT) and integrating multilingual hate speech lexicon features (HurtLex) together into LSTM networks                                                                   |EN-DE, EN-ES, EN-FR, EN-ID, EN-IT, EN-PT                                                                                                                                                                                                                                                                                                                                                       |Macro F1, Accuracy                                               |
|Cross-lingual Offensive Language Identification for Low Resource Languages: The Case of Marathi                                                                   |2021|Christopher M. Homan, Marcos Zampieri, Saurabh Gaikwad, Tharindu Ranasinghe                                                                                                                 |RANLP                                                                           |39       |https://aclanthology.org/2021.ranlp-1.50/                                                                                         |Yes              |https://github.com/tharindudr/MOLD                                                                                            |offensive                                                   |no                 |XLM-R                                                                       |parameter-level                           |create Marathi Offensive Language Dataset (MOLD), and propose zero-shot, few-shot and weight-frozen cross-lingual approaches based on XLM-R, using three source languages (English, Hindi and Bengali) to identify Marathi offensive content                                                                                            |BN-MR, EN-MR, HI-MR                                                                                                                                                                                                                                                                                                                                                                            |macro F1, weighted F1                                            |
|Exposing the limits of Zero-shot Cross-lingual Hate Speech Detection                                                                                              |2021|Debora Nozza                                                                                                                                                                                |ACL-IJCNLP                                                                      |37       |https://aclanthology.org/2021.acl-short.114/                                                                                      |No               |                                                                                                                              |hate speech, misogyny                                       |no                 |XLM-R, mBERT                                                                |parameter-level                           |Explore the limits of cross-lingual hate speech detection based on four different monolingual and cross-lingual learning settings by fine-tuning mBERT / XLM-R                                                                                                                                                                          |EN-ES, EN-IT, ES-IT                                                                                                                                                                                                                                                                                                                                                                            |macro F1                                                         |
|Multilingual Offensive Language Identification for Low-resource Languages                                                                                         |2021|Marcos Zampieri, Tharindu Ranasinghe                                                                                                                                                        |ACM Trans. on Asian and Low-Resource Language Information Processing            |35       |https://dl.acm.org/doi/10.1145/3457610                                                                                            |Yes              |https://github.com/TharinduDR/DeepOffense                                                                                     |offensive                                                   |no                 |XLM-R, mBERT                                                                |parameter-level                           |propose a cross-lingual approach to train XLM-R and mBERT classifier on source language (English) and save model weights to initialise the model for target language                                                                                                                                                                    |EN-AR, EN-BN, EN-EL, EN-ES, EN-HI, EN-TR                                                                                                                                                                                                                                                                                                                                                       |F1, macro F1                                                     |
|Online Multilingual Hate Speech Detection: Experimenting with Hindi and English Social Media                                                                      |2021|Arkaitz Zubiaga, Neeraj Vashistha                                                                                                                                                           |MDPI Information                                                                |30       |https://www.mdpi.com/2078-2489/12/1/5                                                                                             |Yes              |https://github.com/neerajvashistha/online-hate-speech-recog                                                                   |hate speech                                                 |yes                |BERT, CNN-LSTM, LR                                                          |data-level, parameter-level               |Experimented with several models including LR, CNN-LSTM, and BERT to build a multilingual system trained on code-switched datasets in English and Hindi by adopting a transfer learning approach.                                                                                                                                       |EN-HI                                                                                                                                                                                                                                                                                                                                                                                          |F1                                                               |
|MUDES: Multilingual Detection of Offensive Spans                                                                                                                  |2021|Marcos Zampieri, Tharindu Ranasinghe                                                                                                                                                        |NAACL-HLT                                                                       |24       |https://aclanthology.org/2021.naacl-demos.17/                                                                                     |Yes              |https://github.com/tharindudr/MUDES                                                                                           |offensive                                                   |no                 |XLM-R                                                                       |parameter-level                           |propose a multilingual framework MUDES based on XLM-R, and fine-tune MUDES on English source data and evaluate the model on two target datasets in Danish and Greek                                                                                                                                                                     |EN-DA, EN-EL                                                                                                                                                                                                                                                                                                                                                                                   |macro F1                                                         |
|Offensive Language Identification in Low-resourced Code-mixed Dravidian languages using Pseudo-labeling                                                           |2021|Adeep Hande, Anbukkarasi Sampath, Bharathi Raja Chakravarthi, Durairaj Thenmozhi, Karthik Puranik, Kogilavani Shanmugavadivel, Konthala Yasaswini, Ruba Priyadharshini, Sajeetha Thavareesan|Arxiv                                                                           |20       |https://arxiv.org/abs/2108.12177                                                                                                  |Yes              |https://github.com/adeepH/Dravidian-OLI                                                                                       |offensive                                                   |yes                |DistilmBERT, IndicBERT, MuRIL(MU) - multilingual Indic, ULMFiT, XLM-R, mBERT|data-level, parameter-level               |construct a transliterated dataset based on code-mixed texts in Kannada, Malayalam and Tamil with pseudo-labels generated by BERT-based models, and fine-tune the pre-trained model on both newly constructed and code-mixed datasets.                                                                                                  |EN-KN, EN-ML, EN-TA                                                                                                                                                                                                                                                                                                                                                                            |Weighted Precision, Weighted Recall, and Weighted F1-scores      |
|Benchmarking Multi-Task Learning for Sentiment Analysis and Offensive Language Identification in Under-Resourced Dravidian Languages                              |2021|Adeep Hande, Bharathi Raja Chakravarthi, Prassana Kumar Kumaresan, Rahul Ponnusamy, Ruba Priyadharshini, Sajeetha Thavareesan, Siddhanth U Hegde                                            |Arxiv                                                                           |19       |https://arxiv.org/abs/2108.03867                                                                                                  |Yes              |https://github.com/SiddhanthHegde/Dravidian-MTL-Benchmarking                                                                  |offensive                                                   |yes                |ALBERT, BERT, CharacterBERT, DistilBERT, RoBERTa, XLM, XLM-R, XLNet         |parameter-level                           |propose a multi-task learning approach based on sentiment analysis and offensive language identification tasks using diverse pre-trained multilingual models                                                                                                                                                                            |EN-KN, EN-ML, EN-TA                                                                                                                                                                                                                                                                                                                                                                            |F1                                                               |
|Cross-Lingual Transfer Learning for Hate Speech Detection                                                                                                         |2021|Alexander Fraser, Irina Bigoulaeva, Viktor Hangya                                                                                                                                           |ITEDI Workshop                                                                  |16       |https://aclanthology.org/2021.ltedi-1.3/                                                                                          |No               |                                                                                                                              |hate speech                                                 |no                 |CNN, LSTM, SVM                                                              |feature-level, parameter-level            |propose a ensemble-based cross-lingual approach by leveraging bilingual word embeddings to train four neural classifiers (CNN/BiLSTM) on English dataset, generating pseudo labels for two unlabelled German datasets by an ensemble of four trained models, and fine-tuning them on bootstrapping German datasets                      |EN-DE                                                                                                                                                                                                                                                                                                                                                                                          |macro F1                                                         |
|An Evaluation of Multilingual Offensive Language Identification Methods for the Languages of India                                                                |2021|Marcos Zampieri, Tharindu Ranasinghe                                                                                                                                                        |MDPI Information                                                                |16       |https://www.mdpi.com/2078-2489/12/8/306                                                                                           |No               |                                                                                                                              |offensive                                                   |yes                |XLM-R                                                                       |parameter-level                           |propose different cross-lingual approaches in zero-shot, few-shot and multi-source training settings across six languages from the two most widely spoken language families in India                                                                                                                                                    |EN-BN, EN-HI, EN-KN, EN-ML, EN-TA, EN-UR                                                                                                                                                                                                                                                                                                                                                       |macro F1                                                         |
|Zero-shot Cross-lingual Content Filtering: Offensive Language and Hate Speech Detection                                                                           |2021|Andraž Pelicon, Blaž Škrlj, Matej Martinc, Matthew Purver, Ravi Shekhar, Senja Pollak                                                                                                       |EACL Hackashop                                                                  |14       |https://aclanthology.org/2021.hackashop-1.5/                                                                                      |Yes              |http:// classify.ijs.si/ml_hate_speech/ml_bert                                                                                |hate speech, offensive                                      |no                 |MLP, mBERT                                                                  |feature-level, parameter-level            |propose a zero-shot cross-lingual transfer approach by training mBERT or LASER embeddings in multilayer perceptron classifier on English dataset and evaluate models on five different target languages                                                                                                                                 |EN-AR, EN-DE, EN-ES, EN-ID                                                                                                                                                                                                                                                                                                                                                                     |accuracy, macro F1                                               |
|Investigating cross-lingual training for offensive language detection                                                                                             |2021|Andraž Pelicon, Blaž Škrlj, Matthew Purver, Ravi Shekhar, Senja Pollak                                                                                                                      |PeerJ Computer Science                                                          |14       |https://peerj.com/articles/cs-559/                                                                                                |Yes              |https://github.com/EMBEDDIA/cross-lingual_training_for_offensive_language_detection                                           |offensive                                                   |no                 |cseBERT, mBERT                                                              |parameter-level                           |propose cross-lingual intermediate training regimes by training mBERT / cseBERT on one or more non-target languages (English, Slovenian and Arabic) and then fine-tuning the trained model on different amounts (from 0 to 100%) of the five target languages                                                                           |AR-DE, AR-HR, AR-SL, EN-AR, EN-DE, EN-HR, EN-SL, SL-DE, SL-HR                                                                                                                                                                                                                                                                                                                                  |macro F1                                                         |
|A Study of Multilingual Toxic Text Detection Approaches under Imbalanced Sample Distribution                                                                      |2021|Degen Huang, Guizhe Song, Zhifeng Xiao                                                                                                                                                      |MDPI Information                                                                |12       |https://www.mdpi.com/2078-2489/12/5/205                                                                                           |No               |                                                                                                                              |toxicity                                                    |no                 |XLM-R, mBERT                                                                |data-level, parameter-level               |propose an ensemble strategy to combine different loss functions (BCE and Focal) and multiple pre-trained models (mBERT and XLM-R) into nice combination sets using macro F1 scores as the fusion weights.                                                                                                                              |EN-ES, EN-FR, EN-IT, EN-PT, EN-RU, EN-TR                                                                                                                                                                                                                                                                                                                                                       |macro F1, accuracy, macro-average precision, macro-average recall|
|Efficient Detection of Multilingual Hate Speech by Using Interactive Attention Network with Minimal Human Feedback                                                |2021|Fedor Vitiugin, Hemant Purohit, Yasas Senarath                                                                                                                                              |ACM WebSci                                                                      |6        |https://doi.org/10.1145/3447535.3462495                                                                                           |No               |                                                                                                                              |hate speech                                                 |no                 |LSTM                                                                        |feature-level                             |propose a Multilingual Interactive Attention Network (MLIAN) model by building upon frame semantics theory with attention weights for interpretability and human-in-the-loop paradigm for model adaptability on multilingual corpora                                                                                                    |EN-ES, ES-EN                                                                                                                                                                                                                                                                                                                                                                                   |accuracy, AUC, F1                                                |
|Cross-lingual Capsule Network for Hate Speech Detection in Social Media                                                                                           |2021|Aiqi Jiang, Arkaitz Zubiaga                                                                                                                                                                 |ACM HT                                                                          |5        |https://dl.acm.org/doi/10.1145/3465336.3475102                                                                                    |No               |                                                                                                                              |hate speech, misogyny                                       |no                 |CNN, Capsule Network, LSTM, MLP, SVM, XLM-R, mBERT                          |data-level, parameter-level               |propose a cross-lingual capsule network learning model (CCNL-Ex) by infusing extra hate speech lexical semantics into parallel embeddings of source and translated target data, and then connect them to capsule networks for detection                                                                                                 |EN-ES, EN-IT                                                                                                                                                                                                                                                                                                                                                                                   |macro F1                                                         |
|Detecting Hate Speech in Cross-Lingual and Multi-lingual Settings Using Language Agnostic Representations                                                         |2021|Héctor Allende, Héctor Allende-Cid, Sebastián E. Rodríguez                                                                                                                                  |CIARP                                                                           |5        |https://link.springer.com/chapter/10.1007/978-3-030-93420-0_8                                                                     |No               |                                                                                                                              |hate speech                                                 |no                 |DT, LabSE, RF, SVM, mBERT                                                   |feature-level, parameter-level            |propose zero-shot cross-lingual learning approaches by training SMV-based and tree-based classifiers with LabSE and mBERT Embeddings, or directly training LabSE and mBERT models                                                                                                                                                       |EN-ES, ES-EN                                                                                                                                                                                                                                                                                                                                                                                   |accuracy, F1                                                     |
|Modeling Profanity and Hate Speech in Social Media with Semantic Subspaces                                                                                        |2021|Dana Ruiter, Dietrich Klakow, Thomas Kleinbauer, Vanessa Hahn                                                                                                                               |WOAH Workshop                                                                   |3        |https://aclanthology.org/2021.woah-1.2/                                                                                           |Yes              |www.github.com/uds-lsv/profane_ subspaces                                                                                     |hate speech, profanity                                      |no                 |LDA                                                                         |feature-level                             |propose an approach to learn semantic sub-spaces to model profane language on both word and sentence level representations and evaluate their generalisability on a variety of similar and distant target languages in a zero-shot cross-lingual setting                                                                                |DE-AR, DE-EN, DE-FR                                                                                                                                                                                                                                                                                                                                                                            |macro F1                                                         |
|Cross-lingual Hate Speech Detection using Transformer Models                                                                                                      |2021|Arkaitz Zubiaga, Teodor Tita                                                                                                                                                                |Arxiv                                                                           |3        |https://arxiv.org/pdf/2111.00981.pdf                                                                                              |No               |                                                                                                                              |hate speech                                                 |no                 |XLM-R, mBERT                                                                |parameter-level                           |experiment with fine-tuned altered versions of mBERT and XLM-R to adopt cross-lingual transfer learning on English data for training and French data for testing                                                                                                                                                                        |EN-FR                                                                                                                                                                                                                                                                                                                                                                                          |macro F1, weighted F1                                            |
|Cross-lingual hate speech detection based on multilingual domain-specific word embedding                                                                          |2021|Aymé Arango, Barbara Poblete, Jorge Pérez                                                                                                                                                   |Arxiv                                                                           |2        |https://arxiv.org/abs/2104.14728                                                                                                  |No               |                                                                                                                              |hate speech                                                 |no                 |BERT                                                                        |feature-level                             |propose a hate specific data representation by constructing monolingual vector spaces and utilising bilingual dictionaries for alignment.                                                                                                                                                                                               |EN-ES, EN-IT                                                                                                                                                                                                                                                                                                                                                                                   |BLI-like task                                                    |
|Using Cross Lingual Learning for Detecting Hate Speech in Portuguese                                                                                              |2021|Anderson Almeida Firmino, Anselmo Cardoso de Paiva, Cláudio Souza de Baptista                                                                                                               |DEXA                                                                            |2        |https://link.springer.com/chapter/10.1007/978-3-030-86475-0_17                                                                    |No               |                                                                                                                              |hate speech                                                 |no                 |XLM-R                                                                       |parameter-level                           |explore three cross-lingual learning methods (Zero Shot Transfer, Joint Learning and Cascade Learning) based on XLM-R by using Italian as the source language and Portuguese as the target language.                                                                                                                                    |IT-PT                                                                                                                                                                                                                                                                                                                                                                                          |F1                                                               |
|A Dutch Dataset for Cross-lingual Multilabel Toxicity Detection                                                                                                   |2021|Ben Burtenshaw, Mike Kestemont                                                                                                                                                              |BUCC Workshop                                                                   |2        |https://aclanthology.org/2021.bucc-1.10/                                                                                          |No               |                                                                                                                              |toxicity                                                    |no                 |LSTM, RF, mBERT                                                             |parameter-level                           |propose a random forest ensemble of LSTM using MUSE embeddings and mBERT model on a cross-validated training set with grid-searched parameters                                                                                                                                                                                          |EN-NL                                                                                                                                                                                                                                                                                                                                                                                          |AUC, mean Precision, mean Recall, mean F1                        |
|Deep Learning Models for Multilingual Hate Speech Detection                                                                                                       |2020|Animesh Mukherjee, Binny Mathew, Punyajoy Saha, Sai Saketh Aluru                                                                                                                            |ECML-PKDD                                                                       |121      |https://arxiv.org/abs/2004.06465                                                                                                  |Yes              |https://github.com/punyajoy/DE-LIMIT                                                                                          |hate speech                                                 |no                 |BERT, LR, mBERT                                                             |data-level, feature-level, parameter-level|analyse multilingual hate speech in 9 languages from 16 different sources, and conduct experiments in both monolingual and multilingual settings.                                                                                                                                                                                       |EN-AR, EN-DE, EN-ES, EN-FR, EN-ID, EN-IT, EN-PL, EN-PT                                                                                                                                                                                                                                                                                                                                         |                                                                 |
|Multilingual Offensive Language Identification with Cross-lingual Embeddings                                                                                      |2020|Marcos Zampieri, Tharindu Ranasinghe                                                                                                                                                        |EMNLP                                                                           |105      |https://aclanthology.org/2020.emnlp-main.470/                                                                                     |Yes              |https://github.com/TharinduDR/DeepOffense                                                                                     |aggressiveness, hate speech, offensive                      |no                 |XLM-R                                                                       |parameter-level                           |propose a cross-lingual approach to train XLM-R classifier on source language (English) and save model weights to initialise the model for target languages                                                                                                                                                                             |EN-BN, EN-ES, EN-HI                                                                                                                                                                                                                                                                                                                                                                            |macro F1, weighted F1                                            |
|Misogyny Detection in Twitter: a Multilingual and Cross-Domain Study                                                                                              |2020|Endang Wahyu Pamungkas, Valerio Basile, Viviana Patti                                                                                                                                       |Information Processing and Management                                           |103      |https://www.sciencedirect.com/science/article/abs/pii/S0306457320308554                                                           |No               |                                                                                                                              |misogyny                                                    |no                 |BERT, LSTM, SVM                                                             |data-level, parameter-level               |present a comparative analysis of different cross-lingual models, such as momo-lingual deep learning models with translation, and joint models with multilingual embeddings and lexical hate features.                                                                                                                                  |EN-ES, ES-IT                                                                                                                                                                                                                                                                                                                                                                                   |F1, accuracy                                                     |
|XHATE-999: Analyzing and Detecting Abusive Language Across Domains and Languages                                                                                  |2020|Goran Glavaš, Ivan Vulić, Mladen Karan                                                                                                                                                      |COLING                                                                          |49       |https://aclanthology.org/2020.coling-main.559/                                                                                    |Yes              |https://github.com/codogogo/xhate                                                                                             |abusive                                                     |no                 |XLM-R, mBERT                                                                |parameter-level                           |create a multi-domain and multilingual evaluation dataset (XHate-999) for abusive language detection, and propose a zero-shot cross-lingual approach and a cross-lingual adaptation via intermediate masked language modelling on filtered target data                                                                                  |EN-DE, EN-HR, EN-RU, EN-SQ, EN-TR                                                                                                                                                                                                                                                                                                                                                              |F1                                                               |
|Cross-lingual Zero- and Few-shot Hate Speech Detection utilising frozen Transformer Language Models and AXEL                                                      |2020|Björn Schuller, Fabian Brunn, Lukas Stappen                                                                                                                                                 |Arxiv                                                                           |43       |https://arxiv.org/abs/2004.13850                                                                                                  |No               |                                                                                                                              |hate speech                                                 |no                 |LSTM, XLM                                                                   |parameter-level                           |propose a cross-lingual architecture of using frozen Transformer Language Model (TLM) as the encoder with Attention-Maximum-Average Pooling (AXEL) in zero-shot and few-shot settings                                                                                                                                                   |EN-ES                                                                                                                                                                                                                                                                                                                                                                                          |F1                                                               |
|Hate Speech Detection is Not as Easy as You May Think: A Closer Look at Model Validation (extended version)                                                       |2020|Aymé Arango, Barbara Poblete, Jorge Pérez                                                                                                                                                   |Information Systems                                                             |41       |https://doi.org/10.1016/j.is.2020.101584                                                                                          |Yes              |https://github.com/aymeam/User_distribution_experiments                                                                       |hate speech                                                 |no                 |DT, LSTM                                                                    |data-level, feature-level                 |translate Spanish dataset to English language, and use multilingual word embedding representations MUSE in a Gradient Boosted Decision Tree and a LSTM-based model                                                                                                                                                                      |EN-ES                                                                                                                                                                                                                                                                                                                                                                                          |F1                                                               |
|Hate Speech Detection on Multilingual Twitter Using Convolutional Neural Networks                                                                                 |2020|Aya Elouali, Nadia Elouali, Zakaria Elberrichi                                                                                                                                              |Revue d'Intelligence Artificielle                                               |14       |https://doi.org/10.18280/ria.340111                                                                                               |No               |                                                                                                                              |hate speech                                                 |no                 |CNN                                                                         |data-level, parameter-level               |propose a CNN-based architecture with character level representations and combine datasets in different languages into two versions of multilingual datasets for training and testing                                                                                                                                                   |EN-AR, EN-DE, EN-HI, EN-ID, EN-IT, EN-PT                                                                                                                                                                                                                                                                                                                                                       |accuracy                                                         |
|Code-Switching Patterns Can Be an Effective Route to Improve Performance of Downstream NLP Applications: A Case Study of Humour, Sarcasm and Hate Speech Detection|2020|Animesh Mukherjee, Ayush Suhane, Jasabanta Patro, Srijan Bansal, Vishal Garimella                                                                                                           |ACL                                                                             |14       |https://aclanthology.org/2020.acl-main.96.pdf                                                                                     |No               |                                                                                                                              |hate speech                                                 |yes                |HAN                                                                         |feature-level                             |experiment with Hierarchical Attention Network (HAN) by concatenating switching pattern features between Hindi and English into the last hidden layer of HAN                                                                                                                                                                            |EN-HI                                                                                                                                                                                                                                                                                                                                                                                          |macro F1                                                         |
|Hybrid Emoji-Based Masked Language Models for Zero-Shot Abusive Language Detection                                                                                |2020|Elena Cabrio, Michele Corazza, Sara Tonelli, Serena Villata, Stefano Menini                                                                                                                 |EMNLP                                                                           |13       |https://aclanthology.org/2020.findings-emnlp.84/                                                                                  |No               |                                                                                                                              |abusive                                                     |no                 |XLM                                                                         |feature-level                             |propose a Hybrid Emoji-based Masked Language Model (HE-MLM) to leverage the common information conveyed by emojis across different languages to improve the learned cross-lingual representations of social media texts in a zero-shot setting                                                                                          |EN-DE, EN-ES, EN-IT                                                                                                                                                                                                                                                                                                                                                                            |macro F1                                                         |
|Towards Code-switched Classification Exploiting Constituent Language Resources                                                                                    |2020|Kartikey Pant, Tanvi Dadu                                                                                                                                                                   |AACL SRW                                                                        |5        |https://aclanthology.org/2020.aacl-srw.6/                                                                                         |No               |                                                                                                                              |hate speech                                                 |yes                |RoBERTa, ULMFiT, XLM-R                                                      |data-level, parameter-level               |Convert English-Hindi code-mixed data into the high resource languages (English) with translation and transliteration in a cross-lingual setting based on XLM-R model                                                                                                                                                                   |EN-HI                                                                                                                                                                                                                                                                                                                                                                                          |F1, accuracy                                                     |
|Multilingual and Multi-Aspect Hate Speech Analysis                                                                                                                |2019|Dit-Yan Yeung, Hongming Zhang, Nedjma Ousidhoum, Yangqiu Song, Zizheng Lin                                                                                                                  |EMNLP-IJCNLP                                                                    |190      |https://aclanthology.org/D19-1474/                                                                                                |No               |https://github.com/ HKUST-KnowComp/MLMA_hate_speech                                                                           |hate speech                                                 |no                 |LSTM, Sluice Networks                                                       |parameter-level                           |propose a multitask learning architecture based on Sluice Networks coupled with Babylon and MUSE embeddings                                                                                                                                                                                                                             |EN-AR, EN-FR                                                                                                                                                                                                                                                                                                                                                                                   |macro F1                                                         |
|Cross-domain and Cross-lingual Abusive Language Detection: A Hybrid Approach with Deep Learning and a Multilingual Lexicon                                        |2019|Endang Wahyu Pamungkas, Viviana Patti                                                                                                                                                       |ACL SRW                                                                         |85       |https://aclanthology.org/P19-2051/                                                                                                |Yes              |https://github.com/dadangewp/ACL19-SRW                                                                                        |harassment, hate speech, misogyny, offensive, racism, sexism|no                 |LSTM, SVM                                                                   |data-level, parameter-level               |propose a joint-learning cross-lingual approach to detect hate speech， encoding parallel source and target datasets via multilingual MUSE embeddings and integrating multilingual hate speech lexicon features (HurtLex) together into LSTM networks                                                                                    |EN-ES, EN-IT, ES-EN, ES-IT                                                                                                                                                                                                                                                                                                                                                                     |F1                                                               |
|MC-BERT4HATE: Hate Speech Detection using Multi-channel BERT for Different Languages and Translations                                                             |2019|Hajung Sohn, Hyunju Lee                                                                                                                                                                     |ICDM Workshop                                                                   |55       |https://ieeexplore.ieee.org/document/8955559                                                                                      |No               |                                                                                                                              |hate speech                                                 |no                 |BERT, mBERT                                                                 |data-level, parameter-level               |propose a multi-channel BERT (MC-BERT) model by translating source language to English and Chinese as parallel training or test data inputs and feeding into three versions of BERT (mBERT, English BERT and Chinese BERT)                                                                                                              |EN-DE, EN-ES, EN-IT                                                                                                                                                                                                                                                                                                                                                                            |accuracy, macro F1                                               |
|Mind Your Language: Abuse and Offense Detection for Code-Switched Languages                                                                                       |2019|Kshitij Rajput, Ponnurangam Kumaraguru, Raghav Kapoor, Rajiv Ratn Shah, Roger Zimmermann, Yaman Kumar                                                                                       |AAAI                                                                            |34       |https://ojs.aaai.org/index.php/AAAI/article/view/5112                                                                             |Yes              |https://github.com/yamanksingla/mind-your-language-aaai                                                                       |offensive                                                   |yes                |LSTM                                                                        |parameter-level                           |build a LSTM based model for the code-switched languages Hinglish by saving transferred weights across datasets for further training.                                                                                                                                                                                                   |EN-HI                                                                                                                                                                                                                                                                                                                                                                                          |accuracy                                                         |
|Multilingual and Multitarget Hate Speech Detection in Tweets                                                                                                      |2019|Abhishek Kumar, Farah Benamara Zitoune, Marlène Coulomb-Gully, Patricia Chiril, Véronique Moriceau                                                                                          |PFIA                                                                            |22       |https://aclanthology.org/2019.jeptalnrecital-court.21/                                                                            |No               |                                                                                                                              |hate speech, sexism                                         |no                 |LSTM                                                                        |data-level, feature-level                 |experiment cross-lingual methods from source (English) or merged datasets to target (French) using  multilingual distributional embeddings like Glove bilingual embeddings and self-mapped FastText embeddings                                                                                                                          |EN-FR                                                                                                                                                                                                                                                                                                                                                                                          |macro F1, accuracy                                               |
|Translated vs Non-Translated Method for Multilingual Hate Speech Identification in Twitter                                                                        |2019|Indra Budi, Muhammad Okky Ibrohim                                                                                                                                                           |International Journal on Advanced Science Engineering and Information Technology|12       |http://www.insightsociety.org/ojaseit/index.php/ijaseit/article/view/8123                                                         |No               |                                                                                                                              |hate speech                                                 |yes                |NB, RF, SVM                                                                 |data-level                                |Experiment with the use of machine translation tools to translate test data to English and exploit different traditional models (such as SVM, naive Bayes, and random forest)                                                                                                                                                           |EN-ID                                                                                                                                                                                                                                                                                                                                                                                          |average F1                                                       |
|Detecting Offensive Tweets in Hindi-English Code-Switched Language                                                                                                |2018|Debanjan Mahata, Puneet Mathur, Rajiv Shah, Ramit Sawhney                                                                                                                                   |SocialNLP Workshop                                                              |147      |https://aclanthology.org/W18-3504/                                                                                                |No               |                                                                                                                              |offensive                                                   |yes                |CNN                                                                         |data-level, parameter-level               |create Hindi-English Offensive Tweet (HEOT) dataset, and experiment a transfer learning approach by training CNN on English data and part of translated Hinglish data, and then save transfer weights of CNN to re-train it on Hinglish data from HEOT.                                                                                 |EN-HI                                                                                                                                                                                                                                                                                                                                                                                          |accuracy, macro F1, macro precision, macro recall                |
